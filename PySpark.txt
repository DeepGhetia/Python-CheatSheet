Note:-
1. Pyspark is list and tup friendly and not dic.
2. Packing and Unpacking concept is imp here.
3. pyspark follows strictness (example data types like array and map) and immutability (example creating a new df).
4. In pandas/py boolean and integer are bhai bhai but not in spark due to strictness
5. Alwasy keep in mind while filtering use and/or properly based on requirement
6. Most important withcol and select works same row wise but there is a big dif
    1. withcol works row wuse only and no groupbing/agg allowed also withcol is tough to use with more col at a time
    2. select is dynamic wiorks rowwise also and agg also allowed but not together note that
    
from pyspark.sql import SparkSession
from pyspark.sql.types import *
from pyspark.sql.functions import col, lit, row, upper, lower, cast(astype can also be used but astype is builtin), concat, datediff, concat_ws, when, round
#an hack cocnat() and concat_ws() does implicit casting of int/float etc to stirng before concatenating.

emty df can be created using 4 methods ony - 
    1. emptyRDD()
    2. parallelize()
    3. structtype([])
    4. structype along with structfield\

to convert rdd to df - toDF([col names]) or createDF(pass rdd here, schema=shcema)

#reading file in df - types(csv, json, jdbc, parquet, orc, table)
    1. spark.read.csv('file path', header=True/False, inferSchema=True/False)
    2. spark.read.json('file path')
    3. spark.read.parquet('file path')
    4. spark.read.orc('file path')
    5. spark.read.table('pass view/table name')

#selecting a col
    # 1. df['col']
    # 2. df.col1 \\this is little risky based on the col name
    # 3. col('col1') \\for this we need col form pyspark.sql.functions
    # 4. col('name.firstname') or df['name.firstname'] note here name.firstname mein firstname is nestedcol

#Big Update - 
1. df[col] - while chaining exp this can give error then second chain is applied to first col created as df[col] return col object #hence df[col] not good for chaning
2. col(col1) -  good for chaining and always recomended

lit()
    #lit method is used to add any literal or broadcast the literal to existing df in a new/existing col
    # this can be used with withColumn() method

#withColumn(), withColumnRenamed(), drop()

    1. we can change the data type using this mthod for existing one using cast() from fucntions or astype a default functions
    2. we can create a new col or update the existing one - 
        //we can add lit() - adding literal val
        //perform arithmatic ops
        //perform comparison operation also like boolean filtering to get the values.
        //perform all sorts of operations on diff cols
        //use multiple col methods from functions
            Ex:- df.withColumn('first', col('status')=='Success') -> this is similar to applying boolean logic to a new df col in pandas
        //using when().otherwise() and &,|,~ in then along with () also note - 
            - syntax - when(cond,val).otherwise(val)
        //we can rename the cols using withColumnRenamed(oldname,new name) to rename multiple loop a list of tup and then unpack them using *i
        //also can drop the cols using df.drop(colname) to drop multiple directly pass unpacked list like *lt
    3. Note all these can be done In a single line using chaining concept similar to pandas.

        Ex:- df = (
            df.withColumn("new_col", col("a") + 10)
            .withColumnRenamed("b", "b_new")
            .drop("c")
        )

    4. withColumn()
        //concept of renaming the nested columns is also an impotnat concept as doing it and dropping the main col makes sense.

#df.fillna() -> exactly same as pandas no change but diff
    1. between coalesce for (fewer cols) and fillna (for entire or small df)
    2. repalcing one col value to other can be done using colaesce but not in fillna

#df.select - for selecting the columns/creating new one's same as withColumn
syntax - select(str/col)
1. df.select(col('col'),col('col2')) to select few cols
2. df.select(df.colmuns) to seelct all cols
3. df.select(df.colmuns[1:3]) to select sliced colmuns same as pandas/py
4. we can only use alias/expr() to create a new col in select no other way
5. we also can pass a list of col object basically col()

#expr() from sql.functions module -------- but can take only one expression or one column only new or old ///very imp
complete sql can be written in this expr() function alsong with alias like sql 'as'
Ex:- expr('case when price between 80 and 500 then "decent" when price between 501 and 800 then "good" else "very good" end as comput')

#df.seelctExpr(string only)
1. in selectExpr we have to string only and expression can be directly written in sql expr
Note:- in case of selectexpr we can pass only string but not just a single string for select 3 col that is wrong every new or old col 
       has to be individually passed inside the string.

NULL VALUES - 
1. we can see null values  -
//isNull()/isNotNull()
but cant directly use sum() as strict type also to check every col null values ek saath we have to use list comp in select

2. we can drop null values - df.na.drop(how='any/all', subset=[]) or ddf.dropna(how='any/all', subset=[])
Note:- spark doesnt drop columns as it doesnt have axis param but pandas does

DUPLICATES:-
to check duplicates use rank/groupby  - sql logic as there is none in spark
1. to get only unique recs - having count = 1
2. to get the count of dup recs incluing unique in dup - having count > 1
3. to get only dup values then rownumber() and rank>1

to drop duplicates same as pandas  - 
    1. df.distinct() check for complete rows no other paratmeter it takes
    2. df.dropDuplicates(subset=) // same as pandas but no keep=