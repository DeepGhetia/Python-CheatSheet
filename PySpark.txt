Note:- 
1. Pyspark is list and tup friendly and not dic.
2. Packing and Unpacking concept is imp here.
3. pyspark follows strictness (example data types like array and map) and immutability (example creating a new df).
4. In pandas/py boolean and integer are bhai bhai but not in spark due to strictness
5. Alwasy keep in mind while filtering use and/or properly based on requirement

from pyspark.sql import SparkSession
from pyspark.sql.types import *
from pyspark.sql.functions import col, lit, row, upper, lower, cast(astype can also be used but astype is builtin), concat, datediff, concat_ws, when, round
#an hack cocnat() and concat_ws() does implicit casting of int/float etc to stirng before concatenating.

#selecting a col
    # 1. df['col']
    # 2. df.col1 \\this is little risky based on the col name
    # 3. col('col1') \\for this we need col form pyspark.sql.functions
    # 4. col('name.firstname') or df['name.firstname'] note here name.firstname mein firstname is nestedcol

#Big Update - 
1. df[col] - while chaining exp this can give error then second chain is applied to first col created as df[col] return col object #hence df[col] not good for chaning
2. col(col1) -  good for chaining and always recomended

lit()
    #lit method is used to add any literal or broadcast the literal to existing df in a new/existing col
    # this can be used with withColumn() method

#withColumn(), withColumnRenamed(), drop()

    1. we can change the data type using this mthod for existing one using cast() from fucntions or astype a default functions
    2. we can create a new col or update the existing one - 
        //we can add lit() - adding literal val
        //perform arithmatic ops
        //perform comparison operation also like boolean filtering to get the values.
        //perform all sorts of operations on diff cols
        //use multiple col methods from functions
            Ex:- df.withColumn('first', col('status')=='Success') -> this is similar to applying boolean logic to a new df col in pandas
        //using when().otherwise() and &,|,~ in then along with () also note - 
            - syntax - when(cond,val).otherwise(val)
        //we can rename the cols using withColumnRenamed(oldname,new name) to rename multiple loop a list of tup and then unpack them using *i
        //also can drop the cols using df.drop(colname) to drop multiple directly pass unpacked list like *lt
    3. Note all these can be done In a single line using chaining concept similar to pandas.

        Ex:- df = (
            df.withColumn("new_col", col("a") + 10)
            .withColumnRenamed("b", "b_new")
            .drop("c")
        )

    4. withColumn()
        //concept of renaming the nested columns is also an impotnat concept as doing it and dropping the main col makes sense.

#df.fillna() -> exactly same as pandas no change but diff
    1. between coalesce for (fewer cols) and fillna (for entire or small df)
    2. repalcing one col value to other can be done using colaesce but not in fillna

#df.select - for selecting the columns/creating new one's same as withColumn
syntax - select(str/col)
1. df.select(col('col'),col('col2')) to select few cols
2. df.select(df.colmuns) to seelct all cols
3. df.select(df.colmuns[1:3]) to select sliced colmuns same as pandas/py
4. we can only use alias/expr() to create a new col in select no other way 

#expr() from sql.functions module
complete sql can be written in this expr() function alsong with alias like sql 'as'
Ex:- expr('case when price between 80 and 500 then "decent" when price between 501 and 800 then "good" else "very good" end as comput')

#df.seelctExpr(string only)
1. in selectExpr we have to string only and expression can be directly written in sql expr