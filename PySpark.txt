Note:- 
1. Pyspark is list and tup friendly and not dic.
2. Packing and Unpacking concept is imp here.
3. pyspark follows strictness (example data types like array and map) and immutability (example creating a new df).

from pyspark.sql import SparkSession
from pyspark.sql.types import *
from pyspark.sql.functions import col, lit, row, upper, lower, cast(astype can also be used but astype is builtin), concat, datediff, concat_ws, when, round
#an hack cocnat() and concat_ws() does implicit casting of int/float etc to stirng before concatenating.

#selecting a col
    # 1. df['col']
    # 2. df.col1 \\this is little risky based on the col name
    # 3. col('col1') \\for this we need col form pyspark.sql.functions
    # 4. col('name.firstname') or df['name.firstname'] note here name.firstname mein firstname is nestedcol

#Big Update - 
1. df[col] - while chaining exp this can give error then second chain is applied to first col created as df[col] return col object #hence df[col] not good for chaning
2. col(col1) -  good for chaining and always recomended

lit()
    #lit method is used to add any literal or broadcast the literal to existing df in a new/existing col
    # this can be used with withColumn() method

#withColumn(), withColumnRenamed(), drop()

    1. we can change the data type using this mthod for existing one using cast() from fucntions or astype a default functions
    2. we can create a new col or update the existing one - 
        //we can add lit() - adding literal val
        //perform arithmatic ops
        //perform all sorts of operations on diff cols
        //use multiple col methods from functions
        //using when().otherwise() and &,|,~ in then along with () also note - 
            - syntax - when(cond,val).otherwise(val)
        //we can rename the cols using withColumnRenamed(oldname,new name) to rename multiple loop a list of tup and then unpack them using *i
        //also can drop the cols using df.drop(colname) to drop multiple directly pass unpacked list like *lt
    3. Note all these can be done In a single line using chaining concept similar to pandas.

        Ex:- df = (
            df.withColumn("new_col", col("a") + 10)
            .withColumnRenamed("b", "b_new")
            .drop("c")
        )

    4. withColumnRenamed()
        //concept of renaming the nested columns is also an impotnat concept as doing it and dropping the main col makes sense.

#df.fillna() -> exactly same as pandas no change but diff
    1. between coalesce for (fewer cols) and fillna (for entire or small df)
    2. repalcing one col value to other can be done using colaesce but not in fillna