Default methods
    1. astype()
    2. between()
    3. contains() -> no other options like case=True/False this method is by default case sennsitive also multiple | cannot be used hence use & or | operations in braces.
    4. isin([]) we can also use ~col('col1').isin([]) for interchangigng values (smart move)
    5. like() similar to sql -> One note in pandas we have | inside contains for multiple but in spark we have to col('col1').contains('a') | (col('col2').contains('b'))
    6. substr() -> col('name').substr(pos,len) #here lenght is imp but not in sql
    7. when().otherwise() //but when() is not a built in method
    8. startswith() and endswith()
    9. isNull() and isNotNull() note diff frm pandas notnull() and isnull()
    10. alias() -> note this gives only temp rename not like withColumnRenamed()

Functions/Methods under pyspark.sql.functions - 
2. col() is used to select the col
3. round, ceil, floor, abs ,pow, sqrt
4. for literal we use lit() -> very imp
5. when()/otherwise() -> very imp
6. colaesce() for getting first not null value coalesce(col1,col2,col3) note this is diff from df.colaesce() //repartitioning
7. greatest() and least() works same a coalesce() across columns but gives max and min values also ignores the null means will return the output

Str - 
1. upper, lower, length, substring, initcap() same as (title in py)
2. trim , ltim, rtrim
3. substring (len mandatory)
4. to replace we have to use regexp_replace(col,this,with that)
5. translate() ->
        ex:- translate(col, 'aA', 'bB')
                this is also used to replace but char to char mapping a->A, b->b
6. cocnat() vs concat_ws() 
    //cocnat is used to concat multiple strings with literal but literal explicitly passed but cocnatws not passing extra
    //null in one string in concat means complete string null but not in case of concat_ws
    //note both support implicit converstios from int/float tostring that measn works for other types except strng

    cocnatws (usecase):-
    1. join() in python is applied to a list consider as it as single col we can do same in spark concat_ws(','col('name'))
    2. cocnatws can also be used to concat 2 or more cols

7. for index/find we have instr(colname, 'get pos for this') //if pos not available then 0 but in py find gives -1 and index gives exception
8. split() in spark split(colname,sep)

Datetime - 
before moving to dattime func's we got to cover the datetime data dtypes
1. DataType()
2. TimestampType()
3. TimestampNTZType()

Always Note:- 
1. while working with datetime while reading check properly if string or datetime generally it is string always we need to explicitly cast
2. for filtering we can use string as no other option also implicit converstios happen
3. using other func such as .dt.day we get integer
4. always check the formatting